{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5068, 65231)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ca00d470bd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m ]\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m \u001b[0mpipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"merge\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasepipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import eli5\n",
    "# spaCy is for NLP\n",
    "import spacy\n",
    "# Natural language tool kit\n",
    "import nltk as nl\n",
    "# Pandas is for opening files e.g. CSVs\n",
    "import pandas as pd\n",
    "from sklearn.base import clone\n",
    "# Helpers to speed up and structure machine learning projects with KEYS\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "# Import stop words to remove from our text\n",
    "from nltk.corpus import stopwords\n",
    "from ml_helper.helper import Helper\n",
    "# Import the below models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from scikitplot.metrics import plot_confusion_matrix\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score as metric_scorer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "# Text pre-processing languages\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "#Only need the below to update the list of stopwords from nltk\n",
    "#nl.download('stopwords')\n",
    "%matplotlib inline\n",
    "\n",
    "# Need to understand what all of this is\n",
    "KEYS = {\n",
    "    \"SEED\": 1,\n",
    "    \"DATA_PATH\": \"C:\\\\Users\\\\thoma\\\\Documents\\\\test\\\\data\\\\fake_or_real_news.csv\",\n",
    "    \"TARGET\": \"label\",\n",
    "    \"METRIC\": \"accuracy\",\n",
    "    \"TIMESERIES\": False,\n",
    "    \"SPLITS\": 3,\n",
    "    \"ESTIMATORS\": 150,\n",
    "    \"ITERATIONS\": 500,\n",
    "}\n",
    "\n",
    "hp = Helper(KEYS)\n",
    "\n",
    "# ds = dataset, read the CSV in the Keys\n",
    "ds = pd.read_csv(KEYS[\"DATA_PATH\"], header=0, names=[\"id\", \"title\", \"text\", \"label\"])\n",
    "# Create a train and test ds variables, the test size is 20% and train 80%, randomise using the seed\n",
    "train, test = train_test_split(ds, test_size=0.20, random_state=KEYS[\"SEED\"])\n",
    "\n",
    "# Show the datatypes of the train dataset\n",
    "#print(train.dtypes)\n",
    "# Print the head of the dataset\n",
    "#print(train.head)\n",
    "# Check for missing data using the helper library built in function\n",
    "#print(hp.missing_data(ds))\n",
    "# Print the location row 10 column 2 of training dattestaset (seed has changed order), using iloc (integer location) from pandas\n",
    "#print(train.iloc[1,2])\n",
    "\n",
    "# Merge the title and the body of the text from the dataset being used\n",
    "train[\"merge\"] = train[\"title\"] + train[\"text\"]\n",
    "test[\"merge\"] = test[\"title\"] + test[\"text\"]\n",
    "#print(train.head())\n",
    "\n",
    "# Count vectorizer is a built in sklearn function which converts text to a matrix of token souncts\n",
    "cv = CountVectorizer()\n",
    "# This is the exact same as prototype 1, vectorize the training set to convert articles to matrix of token counts\n",
    "train_data = cv.fit_transform(train[\"merge\"])\n",
    "#print(train_data.shape)\n",
    "# Transform matrix of token counts (from cv) to tf-idf representation - importance of repeated tokens throughout the text is reduced\n",
    "tfidf = TfidfTransformer()\n",
    "train_data = tfidf.fit_transform(train_data)\n",
    "print(train_data.shape)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# Principle Component Analysis not done here but COULD BE SPOKEN ABOUT IN THE BODY OF THE ESSAY?\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Testing of models to see which one is the best\n",
    "\n",
    "basepipe = Pipeline([\n",
    "    ('vect', TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), sublinear_tf=True))\n",
    "])\n",
    "    \n",
    "\n",
    "models = [\n",
    "    {\"name\": \"pac\", \"model\":  PassiveAggressiveClassifier(max_iter=1000, random_state=KEYS[\"SEED\"], tol=1e-3)}\n",
    "]\n",
    "\n",
    "pipe = Pipeline(train[[\"merge\", \"label\"]], models, basepipe)\n",
    "print(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\ml_helper\\helper.py:459: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option(\"max_colwidth\", -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>CV Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Steps</th>\n",
       "      <th>Note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive</td>\n",
       "      <td>0.780 +/- 0.013</td>\n",
       "      <td>13.165662</td>\n",
       "      <td>vect</td>\n",
       "      <td>Base models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.912 +/- 0.005</td>\n",
       "      <td>39.520357</td>\n",
       "      <td>vect</td>\n",
       "      <td>Base models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>0.915 +/- 0.004</td>\n",
       "      <td>60.413722</td>\n",
       "      <td>vect</td>\n",
       "      <td>Base models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pac</td>\n",
       "      <td>0.940 +/- 0.001</td>\n",
       "      <td>43.678312</td>\n",
       "      <td>vect</td>\n",
       "      <td>Base models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pac</td>\n",
       "      <td>0.914 +/- 0.002</td>\n",
       "      <td>4624.131073</td>\n",
       "      <td>strict_lemma_vect</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model         CV Score         Time              Steps  \\\n",
       "0  naive                0.780 +/- 0.013  13.165662    vect                \n",
       "1  logistic_regression  0.912 +/- 0.005  39.520357    vect                \n",
       "2  svm                  0.915 +/- 0.004  60.413722    vect                \n",
       "3  pac                  0.940 +/- 0.001  43.678312    vect                \n",
       "4  pac                  0.914 +/- 0.002  4624.131073  strict_lemma_vect   \n",
       "\n",
       "          Note  \n",
       "0  Base models  \n",
       "1  Base models  \n",
       "2  Base models  \n",
       "3  Base models  \n",
       "4               "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pass in text of ds as well as individual text\n",
    "def strict_tokenizer(text):\n",
    "    # Had to download en_core_web_sm for this to work\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "    return [token.lemma_.lower().strip() + token.pos_ for token in nlp(text)\n",
    "        if \n",
    "            not token.is_stop and not nlp.vocab[token.lemma_].is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.is_digit\n",
    "    ]\n",
    "#^do this to every text\n",
    "\n",
    "# Make new pipeline\n",
    "strict_lemm_pipe = Pipeline([\n",
    "    ('strict_lemma_vect', TfidfVectorizer(analyzer = 'word', max_df=0.99, min_df=0.01, ngram_range=(1,2), tokenizer=strict_tokenizer))\n",
    "])\n",
    "\n",
    "models = [\n",
    "   # {\"name\": \"logistic_regression\", \"model\": LogisticRegression(solver=\"lbfgs\", max_iter=KEYS[\"ITERATIONS\"], random_state=KEYS[\"SEED\"])},\n",
    "    {\"name\": \"pac\", \"model\":  PassiveAggressiveClassifier(max_iter=KEYS[\"ITERATIONS\"], random_state=KEYS[\"SEED\"], tol=1e-3)}\n",
    "]\n",
    "\n",
    "all_scores = hp.pipeline(train[[\"merge\", \"label\"]], models, strict_lemm_pipe, all_scores=all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try lemmatizer less strict without removing punct, digits, stop words\n",
    "def tokenizer(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "    return [token.lemma_.lower().strip() + token.pos_ for token in nlp(text)]\n",
    "\n",
    "lemm_pipe = Pipeline([\n",
    "    ('lemma_vect', TfidfVectorizer(analyzer = 'word', max_df=0.99, min_df=0.01, ngram_range=(1,2), tokenizer=tokenizer))\n",
    "])\n",
    "\n",
    "all_scores = hp.pipeline(train[[\"merge\", \"label\"]], models, lemm_pipe, all_scores=all_scores, quiet = True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
